---
title: "Reference Airline Satisfaction"
output:
  pdf_document: default
  html_document: default
date: "2025-02-27"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Airline Customer Satisfaction: EFA, Logistic Regression, & Visualizations

## Table of Contents

[1. Goals](#1)

[2. Dataset](#2)

[3. Cleaning the dataset](#3)

[4. Correlation Matrix](#4)

[5. Exploratory Factor Analysis](#5)

[6. Exploratory Factor Analysis Interpretation](#6)

[7. Logistic Regression](#7)

[8. Prediction on Test Data](#8)

[9. Results of Logistic Regression](#9)

[10. Visualizations](#10)

### Goals

1. Conduct an exploratory factor analysis to find out latent variables and understand which customer satisfaction factors group together 

2. Which factors affect satisfaction the most? Conduct logistic regression to find this out

3.  Explore the significant variables and visualize them.

### Packages and libraries



```{r}
# install.packages(c("psych","GPArotation"), dependencies = TRUE)
# install.packages("tidyverse")
library(tidyr)
library(dplyr)
library(tidyverse)
library(corrplot)
library(psych)
library(GPArotation)
```
### Dataset

```{r}
ds <- read.csv("airline_passenger_satisfaction.csv")
head(ds)
dim(ds)
str(ds)
```

**Observations**

- The dataset contains 25 columns and 103904 entries from customers.
- Personal information on customers include gender and age.
- Flight habits of customers are recorded as customer type, the type of travel they use, class of flight, and the distance they have travelled.
- Customer satisfaction is registered over several variables 

### Cleaning the dataset

- Let's check the presence of NA's , outliers, and variables that are not useful

```{r}
any(is.na(ds))
summary(ds)
```
**Observations**
- There are 310 missing values in "Arrival.Delay.in.Minutes". I will replace the NA's with its median=0 to normalize it.
- I will remove column X which is the serial number as it's not needed
- Arrival.Delay.in.Minutes is in num datatype, let's change that to int

```{r}
ds$Departure.Delay = as.numeric(ds$Departure.Delay)
ds$Arrival.Delay[is.na(ds$Arrival.Delay)] <-0
any(is.na(ds))
```
Now that the NA's are replaced, let's look at the plot of outliers in numerical variables

```{r}
ds_num<-select_if(ds,is.numeric)%>%dplyr::select(-ID)
ds_num_p<-ds_num %>% gather(variable,values,1:18 )
options(repr.plot.width = 14, repr.plot.height = 8)
ggplot(ds_num_p)+
  geom_boxplot(aes(x=variable,y=values),fill="cadetblue") + 
  facet_wrap(~variable,ncol=6,scales="free") + 
  theme(strip.text.x = element_blank(),
        text = element_text(size=14))  
```

**Observation**

- The variables with outliers are departure, arrival, flight distance, and rating on check-in service. None of these variables can be modified as they contain subjective data. 

**Outcome Variable**

- This is the dependent variable in the dataset. 

```{r}
options(repr.plot.width = 14, repr.plot.height = 8)
ggplot(ds, aes(x=`Satisfaction`))+
  geom_bar(fill="cadetblue", alpha=0.6)+
  geom_text(aes(label=scales::percent((..count..)/sum(..count..))), stat="count", vjust = -0.8, size=6)+
  stat_count(aes(y=..count..,label=paste0("n=",..count..)),geom="text",vjust=1.2,size=5.8,color="gray35")+
  labs(x="Satisfaction", title = "Distribution of Customer Satisfaction")+
  theme(text = element_text(size=16)) 
```

<a id="4"></a> <br>
### Correlation Matrix

Let's take a look at the correlation matrix of the dataset. This will be done in 2 parts:

1) Numerical variables

2) Categorical variables

**Numerical variable Correlation Plot**

```{r}
ds_cor1 <- cor(subset(ds,select = c(Age, Flight.Distance,Departure.Delay,Arrival.Delay)))
summary(ds_cor1)
options(repr.plot.width = 14, repr.plot.height = 8)
corrplot(ds_cor1, na.label = " ", method="color", tl.col = "black", tl.cex = 1)
```

**Categorical variable Correlation Plot**

```{r}
ds_cor2 <- cor(select_if(subset(ds, select=-c(ID, Age, Flight.Distance,Departure.Delay,Arrival.Delay)), is.numeric))
summary(ds_cor2)
options(repr.plot.width = 14, repr.plot.height = 8)
corrplot(ds_cor2, na.label=" ", tl.cex=1, tl.col="black", method="color")
```

**Observations**

- There is a high positive correlation between arrival delay and departure delay. If a flight departs late, it is likely to arrive late as well. 

- Variables such as seat comfort, food and drink, and in-flight entertainment tend to have a positive correlation.
- Variables such as WiFi service, online booking, and gate location tend to have a positive correlation.
- Cleanliness is correlated with gate location, food and drink, seat comfort, and in-flight entertainment. 

<a id="5"></a> <br>
### Exploratory Factor Analysis

#### Step 1: Check factorability

Let's check whether the dataset is factorable using :

- The Kaiser-Meyer-Olkin test 
- Barlett test

**Kaiser-Meyer-Olkin test**

```{r}
KMO(ds_cor2)
```
**Observation**

- The KMO test indicates that the dataset is fit for factor analysis as the overall MSA is 0.78 (Overall MSA should be 0-1.0 & >.60).
- The individual MSA of each variable is > 0.60, which suggests none of them have to be removed for EFA

**Barlett Test**

```{r}
cortest.bartlett(ds_cor2)
```
**Observation**

- Barlett test have a p-value < 0.05, indicating a go-ahead for EFA

#### Step 2: Identify the number of factors

I will use the following methods to determine the number of factors:

- Eigen value method
- Scree plot

**Eigen value method**

Eigen values is a measure that ethat says how much varianceof the variables can a factor explain. 

```{r}
ds.eigen <- data.frame(eigen(ds_cor2)$values)
colnames(ds.eigen)<-"eigen_value"
ds.eigen$type <- ifelse(ds.eigen$`eigen_value`>=1,">=1","<1")

options(repr.plot.width = 14, repr.plot.height = 8)
ggplot(ds.eigen, aes(x= reorder(rownames(ds.eigen),-`eigen_value`) ,y=`eigen_value`, fill=`type`))+
  geom_col()+
  labs(x="Variables", y="Eigen Value")+
  scale_fill_manual(values=c("darkslategray2", "cadetblue3"))+
  ylim(0,4.5)+
theme(text=element_text(size=16))

cat("\n Eigen values of variables with ev>=1: " , "\n \n", 
    ds.eigen$eigen_value[c(1,2,3,4)] , "\n \n",
    "Sum of eigen values where ev >1 : \n" , sum(ds.eigen$eigen_value[c(1,2,3,4)]), "\n ")
```
**Observation**

- Eigen value method indicates a possiblity of 4 factors (or latent variables)
- The sum of eigen values (where eigen value >=1) explains the variance of 9.3 variables.

**Scree plot**

```{r}
options(repr.plot.width = 14, repr.plot.height = 8)
scree(ds_cor2)
```

**Observation**

- Scree plot suggests 3 factors.

#### Step 3: Determining rotation (orthogonal or oblique)

- Orthogonal rotation (eg. varimax) should be used when the correlation between the factors is at least <0.3 (a general rule of thumb). Statistically, it is easier to interpret the results of orthogonally rotated factors because the factors are assumed to be uncorrelated. 

- Oblique rotation (eg. promax) should be used when the correlation between the factors is >0.3. Oblique rotation is less restrictive, i.e., they don't force factors to correlate but they allow it. In cases where factors are not distinct and load common variables, although it becomes complex to explain, may give a better explanation. 

```{r}
# No rotation 
ds.fa1 <- fa(ds_cor2, 3,rotate= "none")
ds.fa1
fa.diagram(ds.fa1)
```

**Observation: no rotation**

- with no rotation, you can see that the variables load onto more than 1 factor. For eg. , Inflight.service has loadings 0.48 and 0.64 for factors 1 and 3, respectively. 

- each factor loads at least 3 variables. 

- Cross loading is present

- To get a simple structure, i.e., variables loading distinctly onto 1 factor, I'd like to see the results after applying a rotation technique. 

```{r}
# Oblique (promax) rotation 
options(repr.plot.width = 14, repr.plot.height = 8)
ds.fa2 <- fa(ds_cor2, 3,rotate="promax")
ds.fa2
fa.diagram(ds.fa2)
```

**Observation: Oblique (promax) rotation**

- Factors 1 and 3 have a correlation of 0.35 

- Factor 2 does not have high correlation with factors 1 and 3

- Since the factors are not highly correlated to one another, promax is not the ideal choice of rotation. Thus, we can move to orthogonal rotation 
```{r}
# Orthogonal (varimax) rotation
options(repr.plot.width = 14, repr.plot.height = 8)
ds.fa3 <- fa(ds_cor2, 3, rotate = "varimax")
ds.fa3
fa.diagram(ds.fa3)
```

**Observation: Orthogonal (varimax) rotation**

- Each factor loads at least 4 variables- this is a positive indication for retaining the factors

- Only check-in service did not load onto any factor

- The loading for each variable onto the factor that it loads is atleast 0.3 (cutoff). 

Now let's take a closer look at the loadings. 

```{r}
print(ds.fa3$loadings, cutoff = 0.3)
```
**Observation**

- Factor 1: food and drink,seat comfort, inflight entertainment, cleanliness

- Factor 2: inflight wifi, departure arrival time convenience, online booking ease, gate location

- Factor 3: on board service, leg room service, baggage handling, inflight services
- online boarding is a low-contribution variable so I chose to leave it out of the factors

Let's check how reliable the factors are using Cronbach's alpha.

#### Reliability of factors using Cronbach's alpha

- to do this, first the factors have to be grouped together

```{r}
if (!require(ltm)) {
  install.packages("ltm")
}
library(ltm)
factor1 <- ds_cor2[, c("Food.and.Drink", "Seat.Comfort", "In.flight.Entertainment", "Cleanliness")]
factor2 <- ds_cor2[,c("In.flight.Wifi.Service","Departure.and.Arrival.Time.Convenience", "Ease.of.Online.Booking","Gate.Location")]
factor3 <- ds_cor2[,c("In.flight.Entertainment", "On.board.Service","Leg.Room.Service","Baggage.Handling","In.flight.Service" )]
cronbach.alpha(factor1)
cronbach.alpha(factor2)
cronbach.alpha(factor3)
```
**Observation**

- the Cronbach's alpha for each factor is more than 0.80. This indicates a good level of factor reliability.

_Caution: Cronbach's alpha > 0.95 could hint to redundancy_ 

<a id="6"></a> <br>
### Exploratory Factor Analysis Interpretation

Based on the variables in the the factors, I interpret the latent variables as:

- Factor 1 - Comfort (food and drink, seat comfort, inflight entertainment, and cleanliness) 

- Factor 2 - Convenience (inflight wifi service,Departure and arrival time, ease of online booking, gate location)

- Factor 3 - Services (entertainment, on board service, leg room service, baggage handling, inflight service)

<a id="7"></a> <br>
### Logistic Regression

The dependent variable in the dataset is binary ("neutral or dissatisfied" and "satisfied"). Therefore, it is fitting to use logistic regression.

```{r}
# install.packages(c("caret","broom","MASS","ROCR"))
library(MASS)
library(ROCR)
library(broom)
library(caret)
```
Before building the logistic regression model, we have to make sure the dataset is ready. To do that:

- First, check if there is a satisfaction imbalance in the DV 

- Next, the DV which should be categotical 

```{r}
table(ds$satisfaction)
str(ds)
```
- The ratio is not 1:2 so we can work with the current balance.

- I decided to not only convert the datatype of satisfaction from character to factor but also to change the column to binary.
Hence, 1 corresponds to satisfied and 0 corresponds to neutral or dissatisfied. 
- I will change all ordinal variables to factors

```{r}
ds <- read.csv("airline_passenger_satisfaction.csv")

ds$Departure.Delay = as.numeric(ds$Departure.Delay)
ds$Arrival.Delay[is.na(ds$Arrival.Delay)] <-0
any(is.na(ds))

## 75% of the sample size
smp_size <- floor(0.8 * nrow(ds))

## set the seed to make your partition reproducible
set.seed(321)
train_ind <- sample(seq_len(nrow(ds)), size = smp_size)

ds_train <- ds[train_ind, ]
ds_test <- ds[-train_ind, ]

ds_train <- subset(ds_train, select=-c(ID))
ds_train$Departure.Delay = as.numeric(ds_train$Departure.Delay)
ds_train$Arrival.Delay[is.na(ds_train$Arrival.Delay)] <-0

ds_train$Satisfaction <- ifelse(ds_train$Satisfaction=="Satisfied",1,0)
ds_train$Satisfaction <- factor(ds_train$Satisfaction, levels = c(0, 1))

ds_train$Gender <- as.factor(ds_train$Gender)
ds_train$Customer.Type <- as.factor(ds_train$Customer.Type)
ds_train$Type.of.Travel <- as.factor(ds_train$Type.of.Travel)
ds_train$Class <- as.factor(ds_train$Class)
ds_train$In.flight.Wifi.Service <- as.factor(ds_train$In.flight.Wifi.Service)
ds_train$Departure.and.Arrival.Time.Convenience <- as.factor(ds_train$Departure.and.Arrival.Time.Convenience)
ds_train$Ease.of.Online.Booking <- as.factor(ds_train$Ease.of.Online.Booking)
ds_train$Gate.Location <- as.factor(ds_train$Gate.Location)
ds_train$Food.and.Drink <- as.factor(ds_train$Food.and.Drink)
ds_train$Online.Boarding <- as.factor(ds_train$Online.Boarding)
ds_train$Seat.Comfort <- as.factor(ds_train$Seat.Comfort)
ds_train$Leg.Room.Service <- as.factor(ds_train$Leg.Room.Service)
ds_train$In.flight.Entertainment <- as.factor(ds_train$In.flight.Entertainment)
ds_train$On.board.Service <- as.factor(ds_train$On.board.Service)
ds_train$Check.in.Service <- as.factor(ds_train$Check.in.Service)
ds_train$In.flight.Service <- as.factor(ds_train$In.flight.Service)
ds_train$Cleanliness <- as.factor(ds_train$Cleanliness)
ds_train$Baggage.Handling <- as.factor(ds_train$Baggage.Handling)

ds_train <- subset(ds_train, On.board.Service != "0")
ds_train$On.board.Service <- droplevels(ds_train$On.board.Service)
ds_train <- subset(ds_train, Check.in.Service != "0")
ds_train$Check.in.Service <- droplevels(ds_train$Check.in.Service)
ds_train <- subset(ds_train, Seat.Comfort != "0")
ds_train$Seat.Comfort <- droplevels(ds_train$Seat.Comfort)

ds_train <- subset(ds_train, In.flight.Service != "0")
ds_train$In.flight.Service <- droplevels(ds_train$In.flight.Service)

ds_train <- subset(ds_train, Gate.Location != "0")
ds_train$Gate.Location <- droplevels(ds_train$Gate.Location)

ds_train <- subset(ds_train, Cleanliness != "0")
ds_train$Cleanliness <- droplevels(ds_train$Cleanliness)

ds_train <- subset(ds_train, In.flight.Entertainment != "0")
ds_train$In.flight.Entertainment <- droplevels(ds_train$In.flight.Entertainment)

str(ds_train)
```
Let's build the logistic regression model with all variables (Model 1)

```{r}
set.seed(123)
model1 <- glm(Satisfaction ~ Gender + Customer.Type + Age + 
                 
                 Type.of.Travel + Class + Flight.Distance + In.flight.Wifi.Service +
                
                 Departure.and.Arrival.Time.Convenience + Ease.of.Online.Booking + 
                 Gate.Location + Food.and.Drink + Online.Boarding + Seat.Comfort +
                 In.flight.Entertainment + On.board.Service + Leg.Room.Service +
                 Baggage.Handling + Check.in.Service + In.flight.Service +
                 Cleanliness 
               + Departure.Delay + Arrival.Delay , 
               data = ds_train, family = "binomial")
summary(model1)
```
**Observations**

- The AIC of Model 1 is 37156

- Variables like gender, flight distance, inflight wifi service, gate location, food and drinks, seat comfort, inflight entertainment, onboard services, legroom service are not significant. 

Let's create a model with significant variables only (Model 2)

```{r}
set.seed(123)
model2 <- glm(Satisfaction ~ Customer.Type + Age + 
                 
                 Type.of.Travel + Class +                 Departure.and.Arrival.Time.Convenience + Ease.of.Online.Booking + 
                 Online.Boarding + Seat.Comfort +
                 In.flight.Entertainment + On.board.Service + Leg.Room.Service +
                 Baggage.Handling + Check.in.Service + In.flight.Service +
                 Cleanliness 
               + Departure.Delay + Arrival.Delay , 
               data = ds_train, family = "binomial")

summary(model2)
```

**Observations**

- The AIC of Model 1 is 50406 which is higher than Model 1. Therefore,we'll keep using Model 1 for prediction. 

### ANOVA test

To be more sure, I'd like to compare the two models using ANOVA test. My NULL hypothesis (H0) is that Model 1 is better than Model 2. Therefore, if p < 0.05, the NULL hypothesis will be rejected , suggesting Model 1 is better. 

```{r}
anova(model1, model2, test="Chisq")
```
**Observations**

- The p-value is <0.05 which is significant. Thus, H0 is thus rejected, and Model 1 is confirmed to be better than Model 2. 

<a id="8"></a> <br>
### Prediction on Test Data 

Okay, now that the trained model is ready, its time to test Model 1 on the test dataset. 

**Step 1**

- Read and process test dataset

```{r}
#ds_test <- ds[-train_ind, ]

ds_test <- subset(ds_test, select=-c(ID))
ds_test$Departure.Delay = as.numeric(ds_test$Departure.Delay)
ds_test$Arrival.Delay[is.na(ds_test$Arrival.Delay)] <-0

ds_test$Satisfaction <- ifelse(ds_test$Satisfaction=="Satisfied",1,0)
ds_test$Satisfaction <- factor(ds_test$Satisfaction, levels = c(0, 1))

ds_test$Gender <- as.factor(ds_test$Gender)
ds_test$Customer.Type <- as.factor(ds_test$Customer.Type)
ds_test$Type.of.Travel <- as.factor(ds_test$Type.of.Travel)
ds_test$Class <- as.factor(ds_test$Class)
ds_test$In.flight.Wifi.Service <- as.factor(ds_test$In.flight.Wifi.Service)
ds_test$Departure.and.Arrival.Time.Convenience <- as.factor(ds_test$Departure.and.Arrival.Time.Convenience)
ds_test$Ease.of.Online.Booking <- as.factor(ds_test$Ease.of.Online.Booking)
ds_test$Gate.Location <- as.factor(ds_test$Gate.Location)
ds_test$Food.and.Drink <- as.factor(ds_test$Food.and.Drink)
ds_test$Online.Boarding <- as.factor(ds_test$Online.Boarding)
ds_test$Seat.Comfort <- as.factor(ds_test$Seat.Comfort)
ds_test$Leg.Room.Service <- as.factor(ds_test$Leg.Room.Service)
ds_test$In.flight.Entertainment <- as.factor(ds_test$In.flight.Entertainment)
ds_test$On.board.Service <- as.factor(ds_test$On.board.Service)
ds_test$Check.in.Service <- as.factor(ds_test$Check.in.Service)
ds_test$In.flight.Service <- as.factor(ds_test$In.flight.Service)
ds_test$Cleanliness <- as.factor(ds_test$Cleanliness)
ds_test$Baggage.Handling <- as.factor(ds_test$Baggage.Handling)


ds_test <- subset(ds_test, On.board.Service != "0")
ds_test$On.board.Service <- droplevels(ds_test$On.board.Service)
ds_test <- subset(ds_test, Check.in.Service != "0")
ds_test$Check.in.Service <- droplevels(ds_test$Check.in.Service)
ds_test <- subset(ds_test, Seat.Comfort != "0")
ds_test$Seat.Comfort <- droplevels(ds_test$Seat.Comfort)

ds_test <- subset(ds_test, In.flight.Service != "0")
ds_test$In.flight.Service <- droplevels(ds_test$In.flight.Service)

ds_test <- subset(ds_test, Gate.Location != "0")
ds_test$Gate.Location <- droplevels(ds_test$Gate.Location)

ds_test <- subset(ds_test, Cleanliness != "0")
ds_test$Cleanliness <- droplevels(ds_test$Cleanliness)

ds_test <- subset(ds_test, In.flight.Entertainment != "0")
ds_test$In.flight.Entertainment <- droplevels(ds_test$In.flight.Entertainment)


str(ds_test)
```

```{r}
for (col in colnames(ds_test)) {
  if (col %in% colnames(ds_train) && is.factor(ds_train[[col]])) {
    print(col)
    ds_test[[col]] <- factor(ds_test[[col]], levels = levels(ds_train[[col]]))
  }
}
str(ds_test)
```

```{r}
summary(ds_test)
```
**Step 2**

- Run prediction on it

```{r}
pred <- predict(model1, type = "response", newdata = ds_test)
summary(pred)
```
**Step 3**

- Find out the accuracy, sensitivity, AUC ROC, etc. of the prediction. We will find out how reliable the logistic regression model is, and how accurately it makes predictions. 

- Unlike linear regression, R-squared value is not present in logistic regression that can explain the proportion of variance in the DV caused by the IVs 

**AUC-ROC**

```{r}
# ROC plot (sensitivity versus specificity)
ROC_pred <- ROCR::prediction(pred, ds_test$Satisfaction)
ROC_perf<- ROCR::performance(ROC_pred, measure = "tpr", x.measure = "fpr")
options(repr.plot.width = 14, repr.plot.height = 8)
plot(ROC_perf, colorize=TRUE, print.cutoffs.at=seq(0,1,by =0.1), text.adj=c(-0.3,1.6), text.cex = 1.2 , main="ROC Curve", xlab="1-Specificity", ylab="Sensitivity")

# AUC of ROC plot
auc <- as.numeric(performance(ROC_pred, measure = "auc")@y.values)
noquote("The AUC ROC value is : ")
auc
noquote("The threshold value for cutoff from the ROC curve appears to be 0.7 ")
```

```{r}
# Confusion matrix
confusion_matrix <- table(ds_test$Satisfaction, pred>0.7)
row.names(confusion_matrix)<- c("neutral or dissatisfied", "satisfied")
noquote("Confusion Matrix")
confusion_matrix
```
**Observations**

The columns are predictions and the rows are actual values. The values are :

- True Negative (TN) = 14300

- False Negative (FN) = 400

- False Positive (FP) = 1587

- True Positive (TP) = 9686


Using the values of the confusion matrix, lets calculate other metrics. 

 - Accuracy (all correct / all) = TP + TN / total
 
 - Misclassification (all incorrect / all) = FP + FN / total
 
 - Precision (true positives / predicted positives) = TP / TP + FP
 
 - Sensitivity (true positives / all actual positives) = TP / TP + FN
 
 - Specificity (true negatives / all actual negatives) =TN / TN + FP

```{r}
tp <- 9686
fp <- 1587
fn <- 400
tn <- 14300
noquote("Accuracy")
(tp+tn)/(tp+tn+fp+fn)
noquote("Misclassification")
(fp+fn)/(tp+tn+fp+fn)
noquote("Precision")
tp/(tp+fp)
noquote("Sensitivity")
tp/(tp+fn)
noquote("Specificity")
tn/(tn+fp)
```
<a id="9"></a> <br>
### Results of Logistic Regression

- The model's prediction accuracy is 92% and misclassification is 7%. Both measures indicate reliability of the model.

- Precision is 86% , perhaps this needs to be improved.  

 - Logistic regression showed that for customer satisfaction, the following factors are significantly positively significant. This means when these variables increase, the probability of satisfaction increases. 
 
    1. Loyal customers
 
    2. Convenient departure and arrival time
 
    3. Ease of booking

- Increase in delay of flight arrival decreases the probability of satisfaction

Now that we know the high contribution variables from logistic regression and the latent variables from factor analysis, lets visualize them .

<a id="10"></a> <br>
### Visualizations

**Customer Ratings Distribution**

```{r}
ds_rating <- subset(ds, select=c("In.flight.Wifi.Service", "Departure.and.Arrival.Time.Convenience","Ease.of.Online.Booking", "Gate.Location", "Food.and.Drink", "Online.Boarding", "Seat.Comfort", "In.flight.Entertainment", "On.board.Service", "Leg.Room.Service", "Baggage.Handling", "Check.in.Service", "In.flight.Service","Cleanliness"))
options(repr.plot.width = 14, repr.plot.height = 8)

ds_rating %>%
  gather()%>%
  ggplot(aes(value, fill = value)) +
    facet_wrap(~ key, scales = "free", ncol=3) +
    geom_histogram(stat="count", col="cadetblue")+
  scale_fill_brewer(palette = "Blues")+
  labs(title="Rating of Airline Services")+
theme(text= element_text(size=14))
```

**Categorical Variables**

```{r}
# Select categorical columns
ds_cat <- subset(ds, select = c("Gender", "Customer.Type", "Type.of.Travel", "Class", "Satisfaction"))

# Convert Satisfaction to a factor
ds_cat$Satisfaction <- as.factor(ds_cat$Satisfaction)

# Reshape data from wide to long format
ds_long <- ds_cat %>%
  pivot_longer(cols = everything(), names_to = "key", values_to = "value")

# Ensure 'key' is a factor
ds_long$key <- as.factor(ds_long$key)

# Plot using ggplot2
ggplot(ds_long, aes(value)) +
  geom_histogram(stat = "count", fill = "cadetblue4", alpha = 0.6) +
  facet_wrap(~ key, ncol = 2, scales = "free") +
  labs(title = "Categorical Variables") +
  stat_count(aes(y = ..count.., label = ..count..), geom = "text", 
             vjust = 1.3, size = 4.2, color = "white") +
  theme(text = element_text(size = 16))

```
**Observations**

- The gender ratio is quite balanced with almost equal numbers of male and female passengers

- Loyal customers are much higher in number than disloyal customers

- While the difference is not too high, more passengers are dissatisfied or neutral than satisfied

- Passengers who travel for business are more than the double of personal travel passengers. This explains why maximum passengers go for business class. 

**Customer Loyalty and Satisfaction**

Do loyal customers report satisfaction more than disloyal customers?

```{r}
options(repr.plot.width = 14, repr.plot.height = 8)
ggplot(ds, aes(x=`Customer.Type`, fill = `Satisfaction`))+
  geom_bar(alpha=0.7)+
  geom_text(aes(label= scales::percent((..count..)/sum(..count..))), stat="count",
        vjust = 1.5, size=5)+
  scale_fill_discrete(name = "Satisfaction Status", labels=c("Neutral or Dissatisfied", "Satisfied"))+
  labs(x="Customer Type", y="Count", title="Customer Satisfaction and Loyalty ")+
  theme(legend.position="right",text= element_text(size=16) )
```

**Observation**

- 39 % loyal customers are satisfied and 42.7 % loyal customers are dissatisfied / neutral. 

- A greater % of disloyal customers (13.9 %) are dissatisfied / neutral compared to disloyal and satisfied customers (4.3 %)

**Convenience of Arrival/Departure and Satisfaction**
With increasing conveneince of arrival/departure time, the number of satisfied customers should gradually increase with the rating. The opposite trend should be observed for dissatisfied customers.

```{r}
options(repr.plot.width = 14, repr.plot.height = 8)
ggplot(ds, aes(x=`Departure.and.Arrival.Time.Convenience`, fill =`Satisfaction`))+
  geom_bar(position="dodge", alpha=0.5)+
  labs(x="Rating", y="Count", title="Convenience of Departure/Arrival Time")+
  scale_fill_discrete(name = "Satisfaction Status", labels=c("Neutral or Dissatisfied", "Satisfied"))+
  theme(legend.position="right", text= element_text( size = 16))
```

**Observation**

- Contrary to my assumption, the number of dissatisfied customers increases with the increase of rating. 

**Customer Satisfaction and Ease of Online Booking**

Since increase in the ease of online booking tends to increase satisfaction, as indicated by logistic regression analysis, the number of satisfied customers should should be higher than dissatisfied/neutral customers for the highest rating (=5) in ease of online booking.

```{r}
options(repr.plot.width = 14, repr.plot.height = 8)
ggplot(ds, aes(x=`Ease.of.Online.Booking`, fill =`Satisfaction`))+
  geom_bar(position="dodge", alpha=0.5)+
  labs(x="Rating", y="Count", title="Ease of online booking rating")+
  scale_fill_discrete(name = "Satisfaction Status", labels=c("Neutral or Dissatisfied", "Satisfied"))+
  theme(legend.position="right", text= element_text( size = 16))
```
**Observation**

- As predicted, the highest rating of 5 has been given by a higher number of satisfied customers to ease of online booking. 

- As the rating decreases, the number of dissatisfied customers increases. 
